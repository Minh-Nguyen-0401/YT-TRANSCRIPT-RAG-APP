{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Data Extraction & Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Transcription Extraction Approach & Storage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import tempfile\n",
    "import subprocess\n",
    "import whisper\n",
    "from unidecode import unidecode\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from src._check_retrieve_transcript import *\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def get_video_title(youtube_url):\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"yt-dlp\", \"--get-title\", youtube_url],\n",
    "            capture_output=True, text=True, check=True, encoding=\"utf-8\"\n",
    "        )\n",
    "        title = result.stdout.strip()\n",
    "        return title\n",
    "    except subprocess.CalledProcessError:\n",
    "        return \"transcription\"\n",
    "\n",
    "def sanitize_title(title):\n",
    "    title = unidecode(title)\n",
    "    sanitized = re.sub(r'[^\\w\\s\\-]', '', title)\n",
    "    sanitized = sanitized.lower().strip().replace(' ', '_')\n",
    "    return sanitized\n",
    "\n",
    "def extract_yt_direct(youtube_url):\n",
    "    try:\n",
    "        url = youtube_url\n",
    "        video_id = re.search(r'.+?v=(.*)',url)[1]\n",
    "        trans = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        list_trans = []\n",
    "\n",
    "        for chunk in trans:\n",
    "            list_trans.append(chunk.get(\"text\"))\n",
    "\n",
    "        trans_fin = \" \".join(list_trans).replace(\"xa0\",\"\")\n",
    "        trans_fin_san = re.sub(r'[^\\w\\s\\-]', '', trans_fin)\n",
    "        trans_fin_san = re.sub(r'\\s+', ' ', trans_fin_san)\n",
    "        return trans_fin_san\n",
    "    except Exception as e:\n",
    "        return \"error\"\n",
    "\n",
    "def extract_transcription(url=None):\n",
    "    \"\"\"\n",
    "    function to extract transcription:\n",
    "        1. First get the title of the video\n",
    "        2. Check if transcript already in place (on github directory as a database)\n",
    "        3. If not, extract transcript directly online\n",
    "        4. If not, extract audio & transcribe\n",
    "        5. Export to github\n",
    "\n",
    "    Args:\n",
    "        url (str, optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        transcription\n",
    "        title\n",
    "    \"\"\"\n",
    "    if url != None:\n",
    "        youtube_url = url\n",
    "    else:\n",
    "        youtube_url = str(input(\"Enter a youtube url: \"))\n",
    "    \n",
    "    # Get video title\n",
    "    title = sanitize_title(get_video_title(youtube_url))\n",
    "    title = title if len(title) < 38 else title[:38]\n",
    "\n",
    "    # Check if transcript already in place\n",
    "    check_trans = check_retrieve_transcript_db(title)\n",
    "    if check_trans == \"File not found\" or len(check_trans)==0:\n",
    "        # Case 1: Can extract transcript directly online\n",
    "        transcription = extract_yt_direct(youtube_url=youtube_url)\n",
    "        if \"error\" not in transcription and len(transcription) != 0:\n",
    "            export_to_github(title, transcription)\n",
    "            return transcription,title\n",
    "\n",
    "        # Case 2: Extract audio & transcribe\n",
    "        else:\n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                audio_file_path = os.path.join(temp_dir, \"audio.mp3\")\n",
    "                \n",
    "                # Download only audio using yt-dlp\n",
    "                subprocess.run([\n",
    "                    \"yt-dlp\",\n",
    "                    \"--extract-audio\",\n",
    "                    \"--audio-format\", \"mp3\",\n",
    "                    \"--output\", audio_file_path,\n",
    "                    \"--ffmpeg-location\", r\"C:\\Users\\ACER\\Downloads\\ffmpeg-master-latest-win64-gpl\\bin\",\n",
    "                    youtube_url\n",
    "                ], check=True)\n",
    "\n",
    "                print(\"Downloaded file path:\", audio_file_path)\n",
    "                print(\"Exists?\", os.path.isfile(audio_file_path))\n",
    "\n",
    "                os.environ[\"PATH\"] += os.pathsep + r\"C:\\Users\\ACER\\Downloads\\ffmpeg-master-latest-win64-gpl\\bin\"\n",
    "                print(\"PATH:\", os.environ[\"PATH\"])\n",
    "                \n",
    "                # Load Whisper model\n",
    "                \"\"\"remember to install cuda version that matches your gpu: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\"\"\"\n",
    "                whisper_model = whisper.load_model(\"base\", device=\"cuda\")           \n",
    "                print(\"Model device:\", whisper_model.device)\n",
    "                \n",
    "                # Transcribe\n",
    "                cur_transcription = whisper_model.transcribe(audio_file_path, fp16=True)[\"text\"].strip()\n",
    "\n",
    "                export_to_github(title, cur_transcription)\n",
    "\n",
    "                return cur_transcription,title\n",
    "    else:\n",
    "        return check_trans.lower(),title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_cleaner(text):\n",
    "    \"\"\"Remove redundant characters that are not alphanumeric\n",
    "\n",
    "    Args:\n",
    "        text (str): input text\n",
    "    \"\"\"\n",
    "    pattern = r'[^a-zA-Z0-9\\s]'\n",
    "    text = re.sub(pattern, ' ', text).strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Remove stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stopwords\n",
    "\n",
    "    Args:\n",
    "        text (str): input text\n",
    "\n",
    "    Returns:\n",
    "        text (str): cleaned text without stopwords\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Preprocessing & Storage Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_document(transcription):\n",
    "    text = transcription\n",
    "    text_documents = [Document(page_content=text)]\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=50)\n",
    "    documents = text_splitter.split_documents(text_documents)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Embedding Model, Pine Cone DB & Store Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(\n",
    "    api_key=os.getenv(\"PINECONE_API_KEY\")\n",
    ")\n",
    "\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "def store_vector_db(title,documents):\n",
    "    title = title.replace(\"_\",\"-\")\n",
    "    index_name = f\"{title}-index\"\n",
    "    if index_name not in str(pc.list_indexes()):\n",
    "        pc.create_index(index_name,dimension = 1536, metric = \"cosine\", \n",
    "                        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "                        deletion_protection=\"disabled\")\n",
    "        index = pc.Index(host = pc.describe_index(index_name).host)\n",
    "        pinecone = PineconeVectorStore.from_documents(documents=documents, embedding=embeddings, index_name=index_name\n",
    "    )\n",
    "        return pinecone\n",
    "    else:\n",
    "        index = pc.Index(host = pc.describe_index(index_name).host)\n",
    "        pinecone = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "        return pinecone\n",
    "\n",
    "def reset_index(title):\n",
    "    index_name = f\"{title}-index\"\n",
    "    index = pc.Index(host= pc.describe_index(index_name).host)\n",
    "    index.delete_index(delete_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ExtractVideoTranscription(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        transcription, title = extract_transcription(X)\n",
    "        return {\"transcription\": transcription, \"title\": title}\n",
    "\n",
    "class CleanTranscription(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        trans = remove_stopwords(text_cleaner(X[\"transcription\"]))\n",
    "        return {\"transcription\": trans, \"title\": X[\"title\"]}\n",
    "\n",
    "class SplitDocuments(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        documents = split_document(X[\"transcription\"])\n",
    "        return {\"documents\": documents, \"title\": X[\"title\"]}\n",
    "\n",
    "class StoreVectorDB(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        title = X[\"title\"]\n",
    "        documents = X[\"documents\"]\n",
    "        store_vector_db(title,documents)\n",
    "        return {\"documents\": documents, \"title\": title}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Data Extraction & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for file data_trans/jensen_huang_--_nvidias_ceo_on_the_nex.txt...\n",
      "200\n",
      "Found file. Downloading...\n"
     ]
    }
   ],
   "source": [
    "preprocessing_pipeline = Pipeline([\n",
    "    (\"extract\", ExtractVideoTranscription()),\n",
    "    (\"clean\", CleanTranscription()),\n",
    "    (\"split\", SplitDocuments()),\n",
    "    (\"store\", StoreVectorDB()),\n",
    "])\n",
    "\n",
    "preprocessed_dict = preprocessing_pipeline.fit_transform(str(input(\"Enter a youtube url: \")))\n",
    "documents, title = preprocessed_dict.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent 1: Context-Based Querying Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "translator = GoogleTranslator(source=\"vi\", target=\"en\")\n",
    "\n",
    "\"\"\"-----------------------------------------------------------------\"\"\"\n",
    "\n",
    "def retrieve_context(input):\n",
    "    if detect(input) != \"en\":\n",
    "        query = translator.translate(input)\n",
    "        print(f\"Translated query: {query}\")\n",
    "    else:\n",
    "        query = input\n",
    "        print(f\"Original text already in English\")\n",
    "    \n",
    "    context = store_vector_db(title,documents) \\\n",
    "                .as_retriever(search_type = \"similarity_score_threshold\",search_kwargs={'score_threshold': 0.4}) \\\n",
    "                .get_relevant_documents(query)\n",
    "    compiled_docu = \" \".join([doc.page_content for doc in context])\n",
    "    print(\"Retrieved Context:\", compiled_docu, \"\\n-----\\n\")\n",
    "    return compiled_docu\n",
    "\n",
    "def query_from_context_main():\n",
    "    template = \"\"\"\n",
    "    Think step by step before answering the question based on the context below. If you can't find the answer in the context below, say that you don't know.\n",
    "\n",
    "    **Context:** {context}\n",
    "\n",
    "    **Question:** {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    retriever_step = RunnableLambda(retrieve_context)\n",
    "    retriever = RunnableParallel(context = retriever_step,\n",
    "                        question=RunnablePassthrough(), \n",
    "                        #  language = RunnablePassthrough()\n",
    "                        )\n",
    "    retriever | prompt | model | parser\n",
    "    return retriever | prompt | model | parser \n",
    "\n",
    "\"\"\"-----------------------------------------------------------------\"\"\"\n",
    "\n",
    "def web_search_tool(query):\n",
    "    web_search_tool = TavilySearchResults(k=3)\n",
    "    results = web_search_tool.invoke(query)\n",
    "    extracted_results = \"\\n\".join([remove_stopwords(re.sub(r\"[^a-zA-Z0-9\\s.,!?]\",\"\",res[\"content\"].replace(\"\\n\",\" \").strip())) for res in results])\n",
    "    return extracted_results\n",
    "\n",
    "def alternative_web_search_context(prev_result):\n",
    "     if \"i don't know\" in prev_result.lower():\n",
    "          return True\n",
    "\n",
    "def build_chain_web():\n",
    "    search_step = RunnableLambda(lambda q: web_search_tool(q))  # Define search tool logic\n",
    "    retriever_web = RunnableParallel(context=search_step, question=RunnablePassthrough())\n",
    "    web_prompt = ChatPromptTemplate.from_template(\"Web search-based context:\\n{context}\\n\\n**Question:** {question}\")\n",
    "    return retriever_web | web_prompt | model | parser\n",
    "\n",
    "\"\"\"-----------------------------------------------------------------\"\"\"\n",
    "\n",
    "def start_conversation():\n",
    "\n",
    "    chain_main = query_from_context_main()\n",
    "    chain_web = build_chain_web()\n",
    "\n",
    "    while True:\n",
    "        query = str(input(\"Ask me a question (type 'exit' to quit): \")).lower()\n",
    "\n",
    "\n",
    "        if query == 'exit':\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "        main_result = []\n",
    "        for s in chain_main.stream(query):\n",
    "            main_result.append(s)\n",
    "            print(s, end =\"\",flush=True)\n",
    "        \n",
    "        if alternative_web_search_context(\"\".join(main_result)):\n",
    "            print(f\"\\nSwitching to web search tool...\")\n",
    "            for s in chain_web.stream(query):\n",
    "                main_result.append(s)\n",
    "                print(s, end =\"\",flush=True)\n",
    "        print(\"\\n\",\"---\"*20)\n",
    "\n",
    "        subquery = str(input(\"Continue? (click Enter): \"))\n",
    "        if subquery.lower() == '':\n",
    "             continue\n",
    "        else:\n",
    "             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text already in English\n",
      "Retrieved Context: jensen first one greatest contributions industry democratized scientific computing nvidia gpus breakthroughs alexnet wasnt supercomputer cloud geforce card simultaneously researchers around world buying geforce gpus architecturally theyre supercomputers building able use discover nextthe breakthrough enjoying today thing happening many different fields im really proud fact weve democratized high performance computing put hands researcher dont go get gigantic funds able research one scientists quantum chemistry said one day learned son working one computer companies silicon valley go buy gaming cards download cuda sdk port quantum chemistry software running ibm supercomputer onto gaming gpu amazed fast wait rest week supercomputer finish could compare results went bought many gpus could training machine learning models days uses nvidia really curious conscious strategy like started think made happen jensen started almost simultaneously three different research teams reached us asking us help accelerate neural network models turned reason trying submit imagenet big competition deep learning came consciousness kind around time lukas year jensen iswhen alexs imagenet lukas must like 2011 maybe jensen yeah going say 2012 2013 anyhow something like anyways alexnet year kind around consciousness around time thing really exciting waswe know computer vision hard alex created neural network trained whole bunch data broken record computer vision experts many nvidia trying using human engineered features giant breakthrough caught lot attention computer vision know one foundations like youve actually really maintained ubiquity market wonder attribute architecture chip software like cuda cudnn something else kind keeps ahead competition jensen well partly company formed properly opportunity always field accelerated computing could go way back computer graphics way since beginning company new way application acceleration domain specific application acceleration computer graphics one scientific computing physics simulations others another image processing example another could argue deep learning yet another different domains applications company started mission mind order accelerated computing domain specific accelerated computing really full stack company understand application nature application youre trying accelerate redesign algorithm way would write algorithm write little code put something together jensen long time get tinker people wonderful thing nvidia 24000 people could tinker little something everybody amount tinkering thats going around company incredible theres phrase say reach friends really see way reach friends company brainstorm little something go try something somebody else theyre brainstorming try something thats guess tinkering scale lukas thats super cool love another question lot people askim curious people originally think nvidia games gamer play video games jensen havent played much games see almost every game goes get benefit collaboration every game company world theyre labs people tell ill run go check play bit last time probablyone favorite games battlefield first came kids teenagers home coming gaming age three us \n",
      "-----\n",
      "\n",
      "I don't know.\n",
      "Switching to web search tool...\n",
      "Nvidia was founded on April 5, 1993.\n",
      " ------------------------------------------------------------\n",
      "Translated query: Who is Jensen?\n",
      "Retrieved Context: jensen first time human history producing manufacturing intelligence like production raw material comes lot genius goes box comes intelligence thats refined lukas youre listening gradient dissent show machine learning real world im host lukas biewald today gradient dissent interviewed guest ive looking forward interviewing quite long time jensen huang ceo founder nvidia youve trained machine learning model youve probably trained nvidia hardware get machine learning talk views future holds super fun interview really hope enjoy lukas right well thanks much collected questions community ton theres questions im sure get im going get questions first jensen okay lukas wanted start number one question wanted ask ive always wondered think almost everyone training machine learning models days uses amazing thing producing intelligence first time human history producing manufacturing intelligence like production raw material comes lot genius goes box comes intelligence thats refined large companies depending us ai intelligence manufactured large scales teams working really really hard keep demand lukas youve running nvidia quite long time curious feel youve changed leader decades running company jensen know youre almost asking wrong person could ask almost anybody else around lukas fair enough experience changed jensen thats easier question 30 years old didnt know anything ceo lot learning job many management techniques really dumb dont use anymore lukas like jensen well alright ill give couple lukas awesome thank jensen list dumb things ive done years quite large could write book manufacturing lukas well thanks much thats really kind im touched appreciate jensen keep great work lukas youre enjoying interviews want learn please click link show notes description find links papers mentioned supplemental material transcription work really hard produce check write little code put something together jensen long time get tinker people wonderful thing nvidia 24000 people could tinker little something everybody amount tinkering thats going around company incredible theres phrase say reach friends really see way reach friends company brainstorm little something go try something somebody else theyre brainstorming try something thats guess tinkering scale lukas thats super cool love another question lot people askim curious people originally think nvidia games gamer play video games jensen havent played much games see almost every game goes get benefit collaboration every game company world theyre labs people tell ill run go check play bit last time probablyone favorite games battlefield first came kids teenagers home coming gaming age three us \n",
      "-----\n",
      "\n",
      "Jensen Huang is the CEO and founder of NVIDIA.\n",
      " ------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "start_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent 2: (Advanced) Summarization Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/18/2024 11:40:31 INFO Loaded model pszemraj/led-large-book-summary to cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from textsum.summarize import Summarizer\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "model_name = \"pszemraj/led-large-book-summary\"\n",
    "summarizer = Summarizer(\n",
    "    model_name_or_path=model_name\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "translator = GoogleTranslator(source=\"vi\", target=\"en\")\n",
    "\n",
    "def fetch_list_pinecone(title) -> pd.DataFrame:\n",
    "    # Define index\n",
    "    title_refined = title.replace(\"_\",\"-\")\n",
    "    index_name = f\"{title_refined}-index\"\n",
    "\n",
    "    index = pc.Index(name = index_name, host = pc.describe_index(index_name).host)\n",
    "\n",
    "    # Define vector_id\n",
    "    id_list = []\n",
    "    for id in index.list():\n",
    "        for i in id:\n",
    "            id_list.append(i)\n",
    "\n",
    "    # Fetch vector list including metadata\n",
    "    fetch_list = index.fetch(ids=id_list)\n",
    "\n",
    "    # Return df of id, text, vector values\n",
    "    list_vec = []\n",
    "    for key, content in fetch_list[\"vectors\"].items():\n",
    "        id = key\n",
    "        text = content[\"metadata\"][\"text\"]\n",
    "        values = content[\"values\"]\n",
    "        list_vec.append([id,text,values])\n",
    "    \n",
    "    df_vec_extracted = pd.DataFrame(list_vec, columns=[\"id\", \"text\", \"values\"])\n",
    "\n",
    "    return df_vec_extracted\n",
    "\n",
    "def preprocess_run_kmeans(df_vec_extracted, n = 8) -> pd.DataFrame:\n",
    "    # Split vector values to a separate df to run kmeans\n",
    "    df_vec_text = df_vec_extracted.iloc[:,:2]\n",
    "\n",
    "    df_vec_val = df_vec_extracted.loc[:,[\"id\",\"values\"]]\n",
    "    df_vec_val = pd.concat([df_vec_val[[\"id\"]], pd.DataFrame(df_vec_val[\"values\"].tolist())],axis=1)    \n",
    "\n",
    "    n_clusters = n\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    df_vec_val[\"cluster\"] = kmeans.fit_predict(df_vec_val.iloc[:, 1:])\n",
    "    df_vec_text_clustered = pd.merge(df_vec_text,df_vec_val[\"cluster\"],right_index=True,left_index=True)\n",
    "    \n",
    "    # Return final clustered results in df form\n",
    "    clustered_texts = df_vec_text_clustered.groupby(\"cluster\")[\"text\"].apply(\" \".join).reset_index()\n",
    "    return clustered_texts\n",
    "\n",
    "def summarize_text(input):\n",
    "    out_str = summarizer.summarize_string(input)\n",
    "    return out_str\n",
    "\n",
    "def pre_summarize_chunk(title):\n",
    "    df_vec_extracted = fetch_list_pinecone(title)\n",
    "    clustered_texts = preprocess_run_kmeans(df_vec_extracted, n = 8)\n",
    "    clustered_texts[\"summarized_text\"] = clustered_texts[\"text\"].apply(lambda x: summarize_text(x))\n",
    "    input_sum=\"; \".join(clustered_texts[\"summarized_text\"].values)\n",
    "    return input_sum\n",
    "\n",
    "def summarize_main(title=title):\n",
    "    summarize_temp = \"\"\"\n",
    "    **Summarize the main points** and organize the information into a coherent summary based on the following context:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Ensure the summary is concise, clear, and covers all key details.\n",
    "    \"\"\"\n",
    "\n",
    "    summarize_prompt = ChatPromptTemplate.from_template(summarize_temp)\n",
    "\n",
    "    input_sum = pre_summarize_chunk(title)\n",
    "\n",
    "    # summarization chain of actions\n",
    "    sum_chain = summarize_prompt | model | parser\n",
    "\n",
    "    sum_chunk = []\n",
    "\n",
    "    for s in sum_chain.stream(input_sum):\n",
    "        sum_chunk.append(s)\n",
    "        print(s, end=\"\", flush=True)\n",
    "    return \"\".join(sum_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f458b0855d4450b69cf06d6bd8912c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a77069a333849de996796d7f4272c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f930bf1247d64e4e82f526c7206e9d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9c51c4f3534a89bb2661d62a5b91a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b9d483113f403bad4fe16816abcac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a15f30d5810427b8142674514eb3b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041178b609e1471695a86ce2451f7ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7a4a77d8e4404199c32d68f0a981dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/18/2024 11:41:53 INFO HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chapter emphasizes the concept of \"networks\" as fundamental to understanding daily life and its various elements, particularly in the fields of artificial intelligence and machine learning. It begins by listing the top 10 significant developments in technology, focusing on their interrelationships and collective impact on our lives. Key advancements discussed include:\n",
      "\n",
      "1. The advent of affordable semiconductor chips, facilitating computational advances.\n",
      "2. Enhanced speed and power in data processing.\n",
      "3. A shift from brute force methods to logic, statistics, and synthetic biology.\n",
      "4. The innovation of the floppy drive, enabling individuals to build homemade supercomputers.\n",
      "\n",
      "The narrative shifts to the industrial revolution’s next phase, highlighting the role of machine learning and artificial intelligence in transforming work and thought processes. It references the pioneering work of Jensen and his company in creating new semiconductor technologies and algorithms that enhance computing capabilities.\n",
      "\n",
      "Lucas expresses particular interest in future applications, such as virtual reality, and mentions his creation—a robot with an avatar capable of language comprehension. The author reflects on the transformative power of machine learning, which turns raw data into neural networks, revolutionizing software development and enabling engineers to innovate across diverse fields like agriculture, medicine, and architecture.\n",
      "\n",
      "Despite the excitement around these advancements, the author critiques the inefficacy of concepts like \"tapeout bonuses,\" which aim to reward employee efforts but often fail due to resource limitations in companies. He argues that successful innovation should focus on creating substantial value rather than merely achieving minimum viable products. Ultimately, the chapter explores how interconnected technological developments are reshaping our understanding of productivity and progress in various domains."
     ]
    }
   ],
   "source": [
    "output = summarize_main(title);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
