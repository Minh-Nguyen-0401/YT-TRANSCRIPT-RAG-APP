{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcription Preprocessing & Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Extract Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import tempfile\n",
    "import subprocess\n",
    "import whisper\n",
    "from unidecode import unidecode\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def get_video_title(youtube_url):\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"yt-dlp\", \"--get-title\", youtube_url],\n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "        title = result.stdout.strip()\n",
    "        return title\n",
    "    except subprocess.CalledProcessError:\n",
    "        return \"transcription\"\n",
    "\n",
    "def sanitize_filename(title):\n",
    "    title = unidecode(title)\n",
    "    sanitized = re.sub(r'[^\\w\\s\\-]', '', title)\n",
    "    sanitized = sanitized.lower().strip().replace(' ', '_')\n",
    "    return sanitized\n",
    "\n",
    "def read_transcription(title):\n",
    "    try:\n",
    "        with open(f\"./data_trans/{title}_transcript.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            cur_transcription = f.read()\n",
    "            return cur_transcription\n",
    "    except Exception as e:\n",
    "        print(f\"There was an error: {e}\")\n",
    "\n",
    "def extract_yt_direct(youtube_url):\n",
    "    try:\n",
    "        url = youtube_url\n",
    "        video_id = re.search(r'.+?v=(.*)',url)[1]\n",
    "        trans = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        list_trans = []\n",
    "\n",
    "        for chunk in trans:\n",
    "            list_trans.append(chunk.get(\"text\"))\n",
    "\n",
    "        trans_fin = \" \".join(list_trans).replace(\"xa0\",\"\")\n",
    "        trans_fin_san = re.sub(r'[^\\w\\s\\-]', '', trans_fin)\n",
    "        trans_fin_san = re.sub(r'\\s+', ' ', trans_fin_san)\n",
    "        return trans_fin_san\n",
    "    except Exception as e:\n",
    "        return \"error\"\n",
    "\n",
    "def extract_transcription(url=None):\n",
    "    if url != None:\n",
    "        youtube_url = url\n",
    "    else:\n",
    "        youtube_url = str(input(\"Enter a youtube url: \"))\n",
    "    \n",
    "    # Get video title\n",
    "    title = sanitize_filename(get_video_title(youtube_url))\n",
    "    title = title[:38] #ensure suitability with pinecone index \n",
    "\n",
    "    # Check if transcript already in place\n",
    "    if not os.path.exists(f\"./data_trans/{title}_transcript.txt\") or len(read_transcription(title))==0:\n",
    "        # Case 1: Can extract transcript directly online\n",
    "        transcription = extract_yt_direct(youtube_url=youtube_url)\n",
    "        if \"error\" not in transcription and len(transcription) != 0:\n",
    "            with open(f\"./data_trans/{title}_transcript.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(transcription)\n",
    "\n",
    "        # Case 2: Extract audio & transcribe\n",
    "        else:\n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                audio_file_path = os.path.join(temp_dir, \"audio.mp3\")\n",
    "                \n",
    "                # Download only audio using yt-dlp\n",
    "                subprocess.run([\n",
    "                    \"yt-dlp\",\n",
    "                    \"--extract-audio\",\n",
    "                    \"--audio-format\", \"mp3\",\n",
    "                    \"--output\", audio_file_path,\n",
    "                    \"--ffmpeg-location\", r\"C:\\Users\\ACER\\Downloads\\ffmpeg-master-latest-win64-gpl\\bin\",\n",
    "                    youtube_url\n",
    "                ], check=True)\n",
    "\n",
    "                print(\"Downloaded file path:\", audio_file_path)\n",
    "                print(\"Exists?\", os.path.isfile(audio_file_path))\n",
    "\n",
    "                os.environ[\"PATH\"] += os.pathsep + r\"C:\\Users\\ACER\\Downloads\\ffmpeg-master-latest-win64-gpl\\bin\"\n",
    "                print(\"PATH:\", os.environ[\"PATH\"])\n",
    "                \n",
    "                # Load Whisper model\n",
    "                \"\"\"remember to install cuda version that matches your gpu: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\"\"\"\n",
    "                whisper_model = whisper.load_model(\"base\", device=\"cuda\")           \n",
    "                print(\"Model device:\", whisper_model.device)\n",
    "                \n",
    "                # Transcribe\n",
    "                cur_transcription = whisper_model.transcribe(audio_file_path, fp16=True)[\"text\"].strip()\n",
    "\n",
    "                with open(f\"./data_trans/{title}_transcript.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(cur_transcription)\n",
    "\n",
    "    return read_transcription(title), title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription,title = extract_transcription()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_document(title):\n",
    "    path_to_doc = f\"./data_trans/{title}_transcript.txt\"\n",
    "    loader = TextLoader(path_to_doc)\n",
    "    text_documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=50)\n",
    "    documents = text_splitter.split_documents(text_documents)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Pine Cone DB & Store Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/16/2024 12:43:51 INFO Discovering subpackages in _NamespacePath(['d:\\\\Study\\\\UNIVERSITY\\\\OTHER COURSES\\\\random coding\\\\Portfolio Projects\\\\000. GITHUB\\\\YOUTUBE-RAG-MODEL\\\\.venv\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "12/16/2024 12:43:51 INFO Looking for plugins in pinecone_plugins.inference\n",
      "12/16/2024 12:43:51 INFO Installing plugin inference into Pinecone\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(\n",
    "    api_key=os.getenv(\"PINECONE_API_KEY\")\n",
    ")\n",
    "\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "def store_vector_db(title,documents):\n",
    "    title = title.replace(\"_\",\"-\")\n",
    "    index_name = f\"{title}-index\"\n",
    "    if index_name not in str(pc.list_indexes()):\n",
    "        pc.create_index(index_name,dimension = 1536, metric = \"cosine\", \n",
    "                        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "                        deletion_protection=\"disabled\")\n",
    "        index = pc.Index(host = pc.describe_index(index_name).host)\n",
    "        pinecone = PineconeVectorStore.from_documents(documents=documents, embedding=embeddings, index_name=index_name\n",
    "    )\n",
    "        return pinecone\n",
    "    else:\n",
    "        index = pc.Index(host = pc.describe_index(index_name).host)\n",
    "        pinecone = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "        return pinecone\n",
    "\n",
    "def reset_index(title):\n",
    "    index_name = f\"{title}-index\"\n",
    "    index = pc.Index(host= pc.describe_index(index_name).host)\n",
    "    index.delete_index(delete_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x2193cb02780>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = split_document(title)\n",
    "\n",
    "store_vector_db(title,documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent 1: Context-Based Querying Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Model Chain Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "translator = GoogleTranslator(source=\"vi\", target=\"en\")\n",
    "\n",
    "\"\"\"-----------------------------------------------------------------\"\"\"\n",
    "\n",
    "def retrieve_context(input):\n",
    "    if detect(input) != \"en\":\n",
    "        query = translator.translate(input)\n",
    "        print(f\"Translated query: {query}\")\n",
    "    else:\n",
    "        query = input\n",
    "        print(f\"Original text already in English\")\n",
    "    \n",
    "    context = store_vector_db(title,documents) \\\n",
    "                .as_retriever(search_type = \"similarity_score_threshold\",search_kwargs={'score_threshold': 0.2}) \\\n",
    "                .get_relevant_documents(query)\n",
    "    compiled_docu = \" \".join([doc.page_content for doc in context])\n",
    "    print(\"Retrieved Context:\", compiled_docu, \"\\n-----\\n\")\n",
    "    return compiled_docu\n",
    "\n",
    "def query_from_context_main():\n",
    "    template = \"\"\"\n",
    "    Think step by step before answering the question based on the context below. If you can't find the answer in the context below, say that you don't know.\n",
    "\n",
    "    **Context:** {context}\n",
    "\n",
    "    **Question:** {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    retriever_step = RunnableLambda(retrieve_context)\n",
    "\n",
    "    retriever = RunnableParallel(context = retriever_step, \n",
    "                                question=RunnablePassthrough(), \n",
    "                                #  language = RunnablePassthrough()\n",
    "                                )\n",
    "    chain = retriever | prompt | model | parser\n",
    "\n",
    "    while True:\n",
    "        query = str(input(\"Ask me a question (type 'exit' to quit): \"))\n",
    "        if query.lower() == 'exit':\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "        result = chain.invoke(query)\n",
    "        print(result)\n",
    "        print(\"\\n\",\"---\"*20)\n",
    "\n",
    "        subquery = str(input(\"Continue? (click Enter): \"))\n",
    "        if subquery.lower() == '':\n",
    "             continue\n",
    "        else:\n",
    "             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text already in English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/16/2024 01:06:30 INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Context: Jensen For the very first time in human history we are producing manufacturing intelligence like production Raw material comes in A lot of genius goes into that box And what comes out is intelligence thats refined Lukas Youre listening to Gradient Dissent a show about machine learning in the real world and Im your host Lukas Biewald Today on Gradient Dissent I interviewed a guest that Ive been looking forward to interviewing for quite a long time This is Jensen Huang who is the CEO and founder of NVIDIA If youve trained a machine learning model youve probably trained it on NVIDIA hardware We get into machine learning and we talk about his views on what the future holds This is a super fun interview and I really hope you enjoy it Lukas All right Well thanks so much for doing this We a little demo They called it toy Jensen at the last GTC keynote Basically its a robot But its a virtual robot otherwise known as an avatar It has computer vision it has speech AI and understands language So on and so forth Im super excited about that because in the future many applicationswe really need to go into the application to experience it whether its a virtual factory or virtual hospital or what not It could be for entertainment like the metaverse and the next era of the internet You want to go into that world And the way to go into that world is through a wormhole called VR We can go into that world But we could also have those agents come out of that world and collaborate with us They would come out through the wormhole called AR and be in our world But otherwise the metaverse do that Jensen First of all one of our greatest contributions to the industry is we democratized scientific computing Because of NVIDIA GPUs the breakthroughs for AlexNet wasnt a supercomputer in the cloud it was a GeForce card Simultaneously researchers around the world were buying GeForce GPUs And because architecturally theyre all the same as the supercomputers were building they were able to use that to discover the nextthe breakthrough that were all enjoying today The same thing is happening in so many different fields And so Im really proud of the fact that weve democratized high-performance computing We put it in the hands of any researcher They dont have to go get gigantic funds to be able to do their research One of the scientists that was in quantum chemistry said to me one day is coming Jensen I dont know about that However if we reframe the problem if we reframe the question just slightly and say Will AI be able to do things that are much better than humans can You and I both know that in fact if you reframe the question that way AI in many many fields are already superhuman And I think that the number of superhuman skills that AI will learn over the course of the next decadeit is quite extraordinary I doubt that there will be many manipulation tasks that are repetitive that robotics wont do better than humans Which is one of the reasons why theres so much work in surgical robotics Their hands will never shake Theyll be able to make the most minute and the most precise of incisions and its perception ability is going to be incredible So I think that in the \n",
      "-----\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/16/2024 01:06:32 INFO HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jensen Huang is the CEO and founder of NVIDIA.\n",
      "Goodbye!\n",
      "Translated query: exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/16/2024 01:06:51 INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Context: surrounded by problems not good news I happen to enjoy that I enjoy solving problems So I completely separate the financial success of the company from the importance of the work and doing impactful work Ive historically always done that whether the company is doing well or badly When we were doing badly particularly during the time when we bet the farm on accelerated computing we wanted every single chip to have the same architecture that I mentioned earlier the pressure on our financial performance was immense But I was equally as enthusiastic then and believed as much in the future as I do today Lukas Thats incredible You dont feel the outside pressure at all or are you able to separate yourself from it Jensen No as a public company youre going to feel a lot of outside pressure Some going to feel a lot of outside pressure Some investors are really artful in expressing their displeasure and criticism and some investors are understandably less patient But its our job to express the reason why were doing what were doing CEOs have to bewe have to be reasoned We have to have a purpose by which were doing something If were clear in expressing why were doing something and our vision for it and we genuinely believe it we genuinely believe it my experience has been that people are willing to give it a shot When we first started our company consumer 3D graphics didnt exist Even APIs for it didnt exist We had to go evangelize that And it took longer than people thought When we moved into accelerated computing for about 15 years it didnt exist It took longer than I thought I out when a chip is ready to be taped out We can create the conditions by which great work can be done We can be good listeners and eliminate obstacles for the team We could be part of the solution by highlighting issues recruiting All kinds of things that we can do to help them reason about priorities help them reduce the scope of their work and try to seek the minimum viable product instead of building such giant things There are a lot of different skills that we couldve instilled into the organization but the one thing that it doesnt really need is a tapeout bonus an achievement bonus Because everybodys trying to do their best Thats one example Lukas Thats a great one What else If youve got others Id love to hear them Jensen Okay Heres another one Well I want to be diplomatic as well economic problem All of it contributes in this geometry-awarebecause you know terrain matters and multi-physicsand we finally might have the necessary algorithms to be able to take a swing at this and build a full-scale digital twin of the earth And hopefully inspire us by giving us a model to test our mitigation strategies and our adaptation strategies and simulate whether the technologies were going to use to absorb carbon or carbon emissions will have the necessary impact a decade two decades four decades from now If not for deep learning and the work that were doing that wouldnt even be possible I wouldnt even imagine doing it Lukas Cool One of the things I wanted to make sure I asked you on a personal level is Ive really admired how youve run the same company for a really long time \n",
      "-----\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/16/2024 01:06:54 INFO HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "query_from_context_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent 2: (Advanced) Summarization Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/16/2024 12:03:01 INFO Loaded model pszemraj/led-large-book-summary to cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from textsum.summarize import Summarizer\n",
    "\n",
    "model_name = \"pszemraj/led-large-book-summary\"\n",
    "summarizer = Summarizer(\n",
    "    model_name_or_path=model_name\n",
    ")\n",
    "\n",
    "def fetch_list_pinecone(title) -> pd.DataFrame:\n",
    "    # Define index\n",
    "    title_refined = title.replace(\"_\",\"-\")\n",
    "    index_name = f\"{title_refined}-index\"\n",
    "\n",
    "    index = pc.Index(name = index_name, host = pc.describe_index(index_name).host)\n",
    "\n",
    "    # Define vector_id\n",
    "    id_list = []\n",
    "    for id in index.list():\n",
    "        for i in id:\n",
    "            id_list.append(i)\n",
    "\n",
    "    # Fetch vector list including metadata\n",
    "    fetch_list = index.fetch(ids=id_list)\n",
    "\n",
    "    # Return df of id, text, vector values\n",
    "    list_vec = []\n",
    "    for key, content in fetch_list[\"vectors\"].items():\n",
    "        id = key\n",
    "        text = content[\"metadata\"][\"text\"]\n",
    "        values = content[\"values\"]\n",
    "        list_vec.append([id,text,values])\n",
    "    \n",
    "    df_vec_extracted = pd.DataFrame(list_vec, columns=[\"id\", \"text\", \"values\"])\n",
    "\n",
    "    return df_vec_extracted\n",
    "\n",
    "def preprocess_run_kmeans(df_vec_extracted, n = 8) -> pd.DataFrame:\n",
    "    # Split vector values to a separate df to run kmeans\n",
    "    df_vec_text = df_vec_extracted.iloc[:,:2]\n",
    "\n",
    "    df_vec_val = df_vec_extracted.loc[:,[\"id\",\"values\"]]\n",
    "    df_vec_val = pd.concat([df_vec_val[[\"id\"]], pd.DataFrame(df_vec_val[\"values\"].tolist())],axis=1)    \n",
    "\n",
    "    n_clusters = n\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    df_vec_val[\"cluster\"] = kmeans.fit_predict(df_vec_val.iloc[:, 1:])\n",
    "    df_vec_text_clustered = pd.merge(df_vec_text,df_vec_val[\"cluster\"],right_index=True,left_index=True)\n",
    "    \n",
    "    # Return final clustered results in df form\n",
    "    clustered_texts = df_vec_text_clustered.groupby(\"cluster\")[\"text\"].apply(\" \".join).reset_index()\n",
    "    return clustered_texts\n",
    "\n",
    "def summarize_text(input):\n",
    "    out_str = summarizer.summarize_string(input)\n",
    "    return out_str\n",
    "\n",
    "def pre_summarize_chunk(title):\n",
    "    df_vec_extracted = fetch_list_pinecone(title)\n",
    "    clustered_texts = preprocess_run_kmeans(df_vec_extracted, n = 8)\n",
    "    clustered_texts[\"summarized_text\"] = clustered_texts[\"text\"].apply(lambda x: summarize_text(x))\n",
    "    input_sum=\"; \".join(clustered_texts[\"summarized_text\"].values)\n",
    "    return input_sum\n",
    "\n",
    "def summarize_main(title=title):\n",
    "    summarize_temp = \"\"\"\n",
    "    **Summarize the main points** and organize the information into a coherent summary based on the following context:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Ensure the summary is concise, clear, and covers all key details.\n",
    "    \"\"\"\n",
    "\n",
    "    summarize_prompt = ChatPromptTemplate.from_template(summarize_temp)\n",
    "\n",
    "    input_sum = pre_summarize_chunk(title)\n",
    "\n",
    "    # summarization chain of actions\n",
    "    sum_chain = summarize_prompt | model | parser\n",
    "    result = sum_chain.invoke(input_sum)\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443787ec5b174237a6ca240cc3ce4fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ba08baac2b4f598a16fe394deb7513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a3226e305143d9ab6feecadd3c7f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b203172d6a4aedac3b958f6152ca75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be536a923b06411d82a068521e39f556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa463b5b18d8458e80d36cf04de105ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/16/2024 12:04:46 INFO HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Author of the Epilogue is addressing community questions, starting with the strategic use of NVIDIA to accelerate machine learning model development. This initiative originated when three research teams sought NVIDIA's assistance for neural network models to compete in the ImageNet competition, coinciding with Alex Net’s record-breaking achievements in computer vision. This collaboration contributed to the emergence of deep learning.\n",
      "In response to whether he plays video games, the Author admits he does not play but actively engages with gaming trends due to NVIDIA's partnerships with various gaming companies. He expresses skepticism about the metaverse being experienced through current head-mounted display technology.\n",
      "NVIDIA’s approach to computing involves creating a universal GPU platform tailored for diverse industries, emphasizing the need for algorithm redesign based on specific applications. The company focuses on solving problems across various domains, utilizing multimodal AI and innovative learning techniques.\n",
      "Unitech, another contributor, aims to democratize scientific computing and enhance understanding of the past and future through artificial intelligence, ultimately lowering barriers for scientists and engineers. Their geometry-aware approach considers the significance of terrain and geography in applications.\n",
      "The chapter critiques traditional CEO roles, advocating for vulnerability and openness to criticism to foster understanding and meaningful work within the company. The Author believes that being surrounded by talented individuals fuels his drive and emphasizes the importance of continuous learning.\n",
      "Looking ahead in artificial intelligence, the Author envisions developments in robotics and the metaverse, enabled by advanced AI techniques. He highlights a mission to create meaningful work that generates substantial societal impact, focusing on the adaptation of AI for practical skills across various domains, including autonomous vehicles and customer service automation. The concept of building a digital twin within the Omniverse framework is also introduced, showcasing the potential for AI to enhance real-world applications.\n"
     ]
    }
   ],
   "source": [
    "output = summarize_main(title);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Author of the Epilogue is addressing community questions, starting with the strategic use of NVIDIA to accelerate machine learning model development. This initiative originated when three research teams sought NVIDIA's assistance for neural network models to compete in the ImageNet competition, coinciding with Alex Net’s record-breaking achievements in computer vision. This collaboration contributed to the emergence of deep learning.\\nIn response to whether he plays video games, the Author admits he does not play but actively engages with gaming trends due to NVIDIA's partnerships with various gaming companies. He expresses skepticism about the metaverse being experienced through current head-mounted display technology.\\nNVIDIA’s approach to computing involves creating a universal GPU platform tailored for diverse industries, emphasizing the need for algorithm redesign based on specific applications. The company focuses on solving problems across various domains, utilizing multimodal AI and innovative learning techniques.\\nUnitech, another contributor, aims to democratize scientific computing and enhance understanding of the past and future through artificial intelligence, ultimately lowering barriers for scientists and engineers. Their geometry-aware approach considers the significance of terrain and geography in applications.\\nThe chapter critiques traditional CEO roles, advocating for vulnerability and openness to criticism to foster understanding and meaningful work within the company. The Author believes that being surrounded by talented individuals fuels his drive and emphasizes the importance of continuous learning.\\nLooking ahead in artificial intelligence, the Author envisions developments in robotics and the metaverse, enabled by advanced AI techniques. He highlights a mission to create meaningful work that generates substantial societal impact, focusing on the adaptation of AI for practical skills across various domains, including autonomous vehicles and customer service automation. The concept of building a digital twin within the Omniverse framework is also introduced, showcasing the potential for AI to enhance real-world applications.\""
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
