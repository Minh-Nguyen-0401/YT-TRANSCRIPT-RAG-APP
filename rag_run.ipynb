{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Data Extraction & Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Transcription Extraction Approach & Storage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import tempfile\n",
    "import subprocess\n",
    "import whisper\n",
    "from unidecode import unidecode\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from src._check_retrieve_transcript import *\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def get_video_title(youtube_url):\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"yt-dlp\", \"--get-title\", youtube_url],\n",
    "            capture_output=True, text=True, check=True, encoding=\"utf-8\"\n",
    "        )\n",
    "        title = result.stdout.strip()\n",
    "        return title\n",
    "    except subprocess.CalledProcessError:\n",
    "        return \"transcription\"\n",
    "\n",
    "def sanitize_title(title):\n",
    "    title = unidecode(title)\n",
    "    sanitized = re.sub(r'[^\\w\\s\\-]', '', title)\n",
    "    sanitized = sanitized.lower().strip().replace(' ', '_')\n",
    "    return sanitized\n",
    "\n",
    "def extract_yt_direct(youtube_url):\n",
    "    try:\n",
    "        url = youtube_url\n",
    "        video_id = re.search(r'.+?v=(.*)',url)[1]\n",
    "        trans = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        list_trans = []\n",
    "\n",
    "        for chunk in trans:\n",
    "            list_trans.append(chunk.get(\"text\"))\n",
    "\n",
    "        trans_fin = \" \".join(list_trans).replace(\"xa0\",\"\")\n",
    "        trans_fin_san = re.sub(r'[^\\w\\s\\-]', '', trans_fin)\n",
    "        trans_fin_san = re.sub(r'\\s+', ' ', trans_fin_san)\n",
    "        return trans_fin_san\n",
    "    except Exception as e:\n",
    "        return \"error\"\n",
    "\n",
    "def extract_transcription(url=None):\n",
    "    \"\"\"\n",
    "    function to extract transcription:\n",
    "        1. First get the title of the video\n",
    "        2. Check if transcript already in place (on github directory as a database)\n",
    "        3. If not, extract transcript directly online\n",
    "        4. If not, extract audio & transcribe\n",
    "        5. Export to github\n",
    "\n",
    "    Args:\n",
    "        url (str, optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        transcription\n",
    "        title\n",
    "    \"\"\"\n",
    "    if url != None:\n",
    "        youtube_url = url\n",
    "    else:\n",
    "        youtube_url = str(input(\"Enter a youtube url: \"))\n",
    "    \n",
    "    # Get video title\n",
    "    title = sanitize_title(get_video_title(youtube_url))\n",
    "    title = title if len(title) < 38 else title[:38]\n",
    "\n",
    "    # Check if transcript already in place\n",
    "    check_trans = check_retrieve_transcript_db(title)\n",
    "    if check_trans == \"File not found\" or len(check_trans)==0:\n",
    "        # Case 1: Can extract transcript directly online\n",
    "        transcription = extract_yt_direct(youtube_url=youtube_url)\n",
    "        if \"error\" not in transcription and len(transcription) != 0:\n",
    "            export_to_github(title, transcription)\n",
    "            return transcription,title\n",
    "\n",
    "        # Case 2: Extract audio & transcribe\n",
    "        else:\n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                audio_file_path = os.path.join(temp_dir, \"audio.mp3\")\n",
    "                \n",
    "                # Download only audio using yt-dlp\n",
    "                subprocess.run([\n",
    "                    \"yt-dlp\",\n",
    "                    \"--extract-audio\",\n",
    "                    \"--audio-format\", \"mp3\",\n",
    "                    \"--output\", audio_file_path,\n",
    "                    \"--ffmpeg-location\", r\"C:\\Users\\ACER\\Downloads\\ffmpeg-master-latest-win64-gpl\\bin\",\n",
    "                    youtube_url\n",
    "                ], check=True)\n",
    "\n",
    "                print(\"Downloaded file path:\", audio_file_path)\n",
    "                print(\"Exists?\", os.path.isfile(audio_file_path))\n",
    "\n",
    "                os.environ[\"PATH\"] += os.pathsep + r\"C:\\Users\\ACER\\Downloads\\ffmpeg-master-latest-win64-gpl\\bin\"\n",
    "                print(\"PATH:\", os.environ[\"PATH\"])\n",
    "                \n",
    "                # Load Whisper model\n",
    "                \"\"\"remember to install cuda version that matches your gpu: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\"\"\"\n",
    "                whisper_model = whisper.load_model(\"base\", device=\"cuda\")           \n",
    "                print(\"Model device:\", whisper_model.device)\n",
    "                \n",
    "                # Transcribe\n",
    "                cur_transcription = whisper_model.transcribe(audio_file_path, fp16=True)[\"text\"].strip()\n",
    "\n",
    "                export_to_github(title, cur_transcription)\n",
    "\n",
    "                return cur_transcription,title\n",
    "    else:\n",
    "        return check_trans,title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_cleaner(text):\n",
    "    \"\"\"Remove redundant characters that are not alphanumeric\n",
    "\n",
    "    Args:\n",
    "        text (str): input text\n",
    "    \"\"\"\n",
    "    pattern = r'[^a-zA-Z0-9\\s]'\n",
    "    text = re.sub(pattern, ' ', text).strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Remove stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stopwords\n",
    "\n",
    "    Args:\n",
    "        text (str): input text\n",
    "\n",
    "    Returns:\n",
    "        text (str): cleaned text without stopwords\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Preprocessing & Storage Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_document(transcription):\n",
    "    text = transcription\n",
    "    text_documents = [Document(page_content=text)]\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=50)\n",
    "    documents = text_splitter.split_documents(text_documents)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Embedding Model, Pine Cone DB & Store Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(\n",
    "    api_key=os.getenv(\"PINECONE_API_KEY\")\n",
    ")\n",
    "\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "def store_vector_db(title,documents):\n",
    "    title = title.replace(\"_\",\"-\")\n",
    "    index_name = f\"{title}-index\"\n",
    "    if index_name not in str(pc.list_indexes()):\n",
    "        pc.create_index(index_name,dimension = 1536, metric = \"cosine\", \n",
    "                        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "                        deletion_protection=\"disabled\")\n",
    "        index = pc.Index(host = pc.describe_index(index_name).host)\n",
    "        pinecone = PineconeVectorStore.from_documents(documents=documents, embedding=embeddings, index_name=index_name\n",
    "    )\n",
    "        return pinecone\n",
    "    else:\n",
    "        index = pc.Index(host = pc.describe_index(index_name).host)\n",
    "        pinecone = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "        return pinecone\n",
    "\n",
    "def reset_index(title):\n",
    "    index_name = f\"{title}-index\"\n",
    "    index = pc.Index(host= pc.describe_index(index_name).host)\n",
    "    index.delete_index(delete_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ExtractVideoTranscription(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        transcription, title = extract_transcription(X)\n",
    "        return {\"transcription\": transcription, \"title\": title}\n",
    "\n",
    "class CleanTranscription(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        trans = remove_stopwords(text_cleaner(X[\"transcription\"]))\n",
    "        return {\"transcription\": trans, \"title\": X[\"title\"]}\n",
    "\n",
    "class SplitDocuments(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        documents = split_document(X[\"transcription\"])\n",
    "        return {\"documents\": documents, \"title\": X[\"title\"]}\n",
    "\n",
    "class StoreVectorDB(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        title = X[\"title\"]\n",
    "        documents = X[\"documents\"]\n",
    "        store_vector_db(title,documents)\n",
    "        return {\"documents\": documents, \"title\": title}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Data Extraction & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for file data_trans/jensen_huang_--_nvidias_ceo_on_the_nex.txt...\n",
      "200\n",
      "Found file. Downloading...\n",
      "Searching for file data_trans/jensen_huang_--_nvidias_ceo_on_the_nex.txt...\n",
      "200\n",
      "Found file. Downloading...\n"
     ]
    }
   ],
   "source": [
    "preprocessing_pipeline = Pipeline([\n",
    "    (\"extract\", ExtractVideoTranscription()),\n",
    "    (\"clean\", CleanTranscription()),\n",
    "    (\"split\", SplitDocuments()),\n",
    "    (\"store\", StoreVectorDB()),\n",
    "])\n",
    "\n",
    "preprocessed_dict = preprocessing_pipeline.fit_transform(str(input(\"Enter a youtube url: \")))\n",
    "documents, title = preprocessed_dict.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent 1: Context-Based Querying Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "translator = GoogleTranslator(source=\"vi\", target=\"en\")\n",
    "\n",
    "\"\"\"-----------------------------------------------------------------\"\"\"\n",
    "\n",
    "def retrieve_context(input):\n",
    "    if detect(input) != \"en\":\n",
    "        query = translator.translate(input)\n",
    "        print(f\"Translated query: {query}\")\n",
    "    else:\n",
    "        query = input\n",
    "        print(f\"Original text already in English\")\n",
    "    \n",
    "    context = store_vector_db(title,documents) \\\n",
    "                .as_retriever(search_type = \"similarity_score_threshold\",search_kwargs={'score_threshold': 0.2}) \\\n",
    "                .get_relevant_documents(query)\n",
    "    compiled_docu = \" \".join([doc.page_content for doc in context])\n",
    "    print(\"Retrieved Context:\", compiled_docu, \"\\n-----\\n\")\n",
    "    return compiled_docu\n",
    "\n",
    "def query_from_context_main():\n",
    "    template = \"\"\"\n",
    "    Think step by step before answering the question based on the context below. If you can't find the answer in the context below, say that you don't know.\n",
    "\n",
    "    **Context:** {context}\n",
    "\n",
    "    **Question:** {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    retriever_step = RunnableLambda(retrieve_context)\n",
    "\n",
    "    retriever = RunnableParallel(context = retriever_step, \n",
    "                                question=RunnablePassthrough(), \n",
    "                                #  language = RunnablePassthrough()\n",
    "                                )\n",
    "    chain = retriever | prompt | model | parser\n",
    "\n",
    "    while True:\n",
    "        query = str(input(\"Ask me a question (type 'exit' to quit): \"))\n",
    "        if query.lower() == 'exit':\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "        for s in chain.stream(query):\n",
    "            print(s, end =\"\",flush=True)\n",
    "        print(\"\\n\",\"---\"*20)\n",
    "\n",
    "        subquery = str(input(\"Continue? (click Enter): \"))\n",
    "        if subquery.lower() == '':\n",
    "             continue\n",
    "        else:\n",
    "             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text already in English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_70512\\2935904538.py:29: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  .get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Context: intelligence second next era ai weve done really good job soft ai thats cloud recommending music recommending movies next item cart forth really incredible thing would really like toif want take ai point people next phase journey ai learn laws physics many worlds challenges whether climate science autonomous vehicles manufacturing whatever ai cant make prediction make prediction obeys laws physics conservation matter conservation energy understand concept synchronous time working within time lot types problems really impossible develop ai unless something essentially virtual world obeys laws physics reason built omniverse built omniverse several things could happen physically based distributed large ability support large models goal several fold one could teach robot well functioning perception speech ai language understanding able technologies things however ultimately whats valuable intelligence ultimately whats valuable skills hire new college grads lots intelligence skills give skills adapting domains lot ways thats essentiallywhat missing right take intelligence technology translate valuable skills valuable skills whether driving autonomous vehicles valuable skills like customer service call centers valuable skills like automated checkout could automated skills like radiology put radiologist right instrument kinds really valuable skills create thats big part energy right take enabling technology translate skills customers industry developers could adapt kinds different domains thats one large scale application artificial intelligence second next era ai weve done know however reframe problem reframe question slightly say ai able things much better humans know fact reframe question way ai many many fields already superhuman think number superhuman skills ai learn course next decadeit quite extraordinary doubt many manipulation tasks repetitive robotics wont better humans one reasons theres much work surgical robotics hands never shake theyll able make minute precise incisions perception ability going incredible think coming years gon na see superhuman ais wont like us many domains activities theyd quite incredible lukas imagine sit youre watching ai help chip manufacturing design better chips youre probably seeing compounding returns think sort thesis behind singularity right sort ai starts create ai see exponential jensen thats exactly right look see exponential jensen thats exactly right look going able build next generation chips without ai thats kind remarkable statement chip design process architectural processtoday 5 worlds top 500 supercomputers company producing software gets shipped ai chips without ai cant produce software runs ai future without ai wouldnt able design chips use run ai thats right circular positive feedback system go turbocharge every confidence next 10 years going see even greater advances necessarily transistor level absolutely computation level lukas concerns aboutas compute gets important advances science theres impact climate even impacts access whos able make scientific discoveries whos able kind make next really exciting company need supercomputer jensen first one greatest contributions industry \n",
      "-----\n",
      "\n",
      "AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning (the acquisition of information and rules for using it), reasoning (using rules to reach approximate or definite conclusions), and self-correction. In the context provided, AI is described as a technology that can perform tasks better than humans in many fields, including music and movie recommendations, autonomous vehicles, and even complex tasks like chip manufacturing design. It has the potential to create superhuman capabilities in various domains, leveraging large models and advanced computational processes. The overarching goal of AI development is to translate valuable skills into applications across different industries, enhancing productivity and innovation.\n",
      " ------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query_from_context_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent 2: (Advanced) Summarization Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/18/2024 11:40:31 INFO Loaded model pszemraj/led-large-book-summary to cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from textsum.summarize import Summarizer\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "model_name = \"pszemraj/led-large-book-summary\"\n",
    "summarizer = Summarizer(\n",
    "    model_name_or_path=model_name\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "translator = GoogleTranslator(source=\"vi\", target=\"en\")\n",
    "\n",
    "def fetch_list_pinecone(title) -> pd.DataFrame:\n",
    "    # Define index\n",
    "    title_refined = title.replace(\"_\",\"-\")\n",
    "    index_name = f\"{title_refined}-index\"\n",
    "\n",
    "    index = pc.Index(name = index_name, host = pc.describe_index(index_name).host)\n",
    "\n",
    "    # Define vector_id\n",
    "    id_list = []\n",
    "    for id in index.list():\n",
    "        for i in id:\n",
    "            id_list.append(i)\n",
    "\n",
    "    # Fetch vector list including metadata\n",
    "    fetch_list = index.fetch(ids=id_list)\n",
    "\n",
    "    # Return df of id, text, vector values\n",
    "    list_vec = []\n",
    "    for key, content in fetch_list[\"vectors\"].items():\n",
    "        id = key\n",
    "        text = content[\"metadata\"][\"text\"]\n",
    "        values = content[\"values\"]\n",
    "        list_vec.append([id,text,values])\n",
    "    \n",
    "    df_vec_extracted = pd.DataFrame(list_vec, columns=[\"id\", \"text\", \"values\"])\n",
    "\n",
    "    return df_vec_extracted\n",
    "\n",
    "def preprocess_run_kmeans(df_vec_extracted, n = 8) -> pd.DataFrame:\n",
    "    # Split vector values to a separate df to run kmeans\n",
    "    df_vec_text = df_vec_extracted.iloc[:,:2]\n",
    "\n",
    "    df_vec_val = df_vec_extracted.loc[:,[\"id\",\"values\"]]\n",
    "    df_vec_val = pd.concat([df_vec_val[[\"id\"]], pd.DataFrame(df_vec_val[\"values\"].tolist())],axis=1)    \n",
    "\n",
    "    n_clusters = n\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    df_vec_val[\"cluster\"] = kmeans.fit_predict(df_vec_val.iloc[:, 1:])\n",
    "    df_vec_text_clustered = pd.merge(df_vec_text,df_vec_val[\"cluster\"],right_index=True,left_index=True)\n",
    "    \n",
    "    # Return final clustered results in df form\n",
    "    clustered_texts = df_vec_text_clustered.groupby(\"cluster\")[\"text\"].apply(\" \".join).reset_index()\n",
    "    return clustered_texts\n",
    "\n",
    "def summarize_text(input):\n",
    "    out_str = summarizer.summarize_string(input)\n",
    "    return out_str\n",
    "\n",
    "def pre_summarize_chunk(title):\n",
    "    df_vec_extracted = fetch_list_pinecone(title)\n",
    "    clustered_texts = preprocess_run_kmeans(df_vec_extracted, n = 8)\n",
    "    clustered_texts[\"summarized_text\"] = clustered_texts[\"text\"].apply(lambda x: summarize_text(x))\n",
    "    input_sum=\"; \".join(clustered_texts[\"summarized_text\"].values)\n",
    "    return input_sum\n",
    "\n",
    "def summarize_main(title=title):\n",
    "    summarize_temp = \"\"\"\n",
    "    **Summarize the main points** and organize the information into a coherent summary based on the following context:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Ensure the summary is concise, clear, and covers all key details.\n",
    "    \"\"\"\n",
    "\n",
    "    summarize_prompt = ChatPromptTemplate.from_template(summarize_temp)\n",
    "\n",
    "    input_sum = pre_summarize_chunk(title)\n",
    "\n",
    "    # summarization chain of actions\n",
    "    sum_chain = summarize_prompt | model | parser\n",
    "\n",
    "    sum_chunk = []\n",
    "\n",
    "    for s in sum_chain.stream(input_sum):\n",
    "        sum_chunk.append(s)\n",
    "        print(s, end=\"\", flush=True)\n",
    "    return \"\".join(sum_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f458b0855d4450b69cf06d6bd8912c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a77069a333849de996796d7f4272c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f930bf1247d64e4e82f526c7206e9d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9c51c4f3534a89bb2661d62a5b91a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b9d483113f403bad4fe16816abcac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a15f30d5810427b8142674514eb3b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041178b609e1471695a86ce2451f7ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7a4a77d8e4404199c32d68f0a981dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/18/2024 11:41:53 INFO HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chapter emphasizes the concept of \"networks\" as fundamental to understanding daily life and its various elements, particularly in the fields of artificial intelligence and machine learning. It begins by listing the top 10 significant developments in technology, focusing on their interrelationships and collective impact on our lives. Key advancements discussed include:\n",
      "\n",
      "1. The advent of affordable semiconductor chips, facilitating computational advances.\n",
      "2. Enhanced speed and power in data processing.\n",
      "3. A shift from brute force methods to logic, statistics, and synthetic biology.\n",
      "4. The innovation of the floppy drive, enabling individuals to build homemade supercomputers.\n",
      "\n",
      "The narrative shifts to the industrial revolution’s next phase, highlighting the role of machine learning and artificial intelligence in transforming work and thought processes. It references the pioneering work of Jensen and his company in creating new semiconductor technologies and algorithms that enhance computing capabilities.\n",
      "\n",
      "Lucas expresses particular interest in future applications, such as virtual reality, and mentions his creation—a robot with an avatar capable of language comprehension. The author reflects on the transformative power of machine learning, which turns raw data into neural networks, revolutionizing software development and enabling engineers to innovate across diverse fields like agriculture, medicine, and architecture.\n",
      "\n",
      "Despite the excitement around these advancements, the author critiques the inefficacy of concepts like \"tapeout bonuses,\" which aim to reward employee efforts but often fail due to resource limitations in companies. He argues that successful innovation should focus on creating substantial value rather than merely achieving minimum viable products. Ultimately, the chapter explores how interconnected technological developments are reshaping our understanding of productivity and progress in various domains."
     ]
    }
   ],
   "source": [
    "output = summarize_main(title);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
