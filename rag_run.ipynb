{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Data Extraction & Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Transcription Extraction Approach & Storage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import tempfile\n",
    "import subprocess\n",
    "import whisper\n",
    "from unidecode import unidecode\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from src._check_retrieve_transcript import *\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def get_video_title(youtube_url):\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"yt-dlp\", \"--get-title\", youtube_url],\n",
    "            capture_output=True, text=True, check=True, encoding=\"utf-8\"\n",
    "        )\n",
    "        title = result.stdout.strip()\n",
    "        return title\n",
    "    except subprocess.CalledProcessError:\n",
    "        return \"transcription\"\n",
    "\n",
    "def sanitize_title(title):\n",
    "    title = unidecode(title)\n",
    "    sanitized = re.sub(r'[^\\w\\s\\-]', '', title)\n",
    "    sanitized = sanitized.lower().strip().replace(' ', '_')\n",
    "    return sanitized\n",
    "\n",
    "def extract_yt_direct(youtube_url):\n",
    "    try:\n",
    "        url = youtube_url\n",
    "        video_id = re.search(r'.+?v=(.*)',url)[1]\n",
    "        trans = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        list_trans = []\n",
    "\n",
    "        for chunk in trans:\n",
    "            list_trans.append(chunk.get(\"text\"))\n",
    "\n",
    "        trans_fin = \" \".join(list_trans).replace(\"xa0\",\"\")\n",
    "        trans_fin_san = re.sub(r'[^\\w\\s\\-]', '', trans_fin)\n",
    "        trans_fin_san = re.sub(r'\\s+', ' ', trans_fin_san)\n",
    "        return trans_fin_san\n",
    "    except Exception as e:\n",
    "        return \"error\"\n",
    "\n",
    "def extract_transcription(url=None):\n",
    "    \"\"\"\n",
    "    function to extract transcription:\n",
    "        1. First get the title of the video\n",
    "        2. Check if transcript already in place (on github directory as a database)\n",
    "        3. If not, extract transcript directly online\n",
    "        4. If not, extract audio & transcribe\n",
    "        5. Export to github\n",
    "\n",
    "    Args:\n",
    "        url (str, optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        transcription\n",
    "        title\n",
    "    \"\"\"\n",
    "    if url != None:\n",
    "        youtube_url = url\n",
    "    else:\n",
    "        youtube_url = str(input(\"Enter a youtube url: \"))\n",
    "    \n",
    "    # Get video title\n",
    "    title = sanitize_title(get_video_title(youtube_url))\n",
    "    title = title if len(title) < 38 else title[:38]\n",
    "\n",
    "    # Check if transcript already in place\n",
    "    check_trans = check_retrieve_transcript_db(title)\n",
    "    if check_trans == \"File not found\" or len(check_trans)==0:\n",
    "        # Case 1: Can extract transcript directly online\n",
    "        transcription = extract_yt_direct(youtube_url=youtube_url)\n",
    "        if \"error\" not in transcription and len(transcription) != 0:\n",
    "            export_to_github(title, transcription)\n",
    "            return transcription,title\n",
    "\n",
    "        # Case 2: Extract audio & transcribe\n",
    "        else:\n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                audio_file_path = os.path.join(temp_dir, \"audio.mp3\")\n",
    "                \n",
    "                # Download only audio using yt-dlp\n",
    "                subprocess.run([\n",
    "                    \"yt-dlp\",\n",
    "                    \"--extract-audio\",\n",
    "                    \"--audio-format\", \"mp3\",\n",
    "                    \"--output\", audio_file_path,\n",
    "                    \"--ffmpeg-location\", r\"C:\\Users\\ACER\\Downloads\\ffmpeg-master-latest-win64-gpl\\bin\",\n",
    "                    youtube_url\n",
    "                ], check=True)\n",
    "\n",
    "                print(\"Downloaded file path:\", audio_file_path)\n",
    "                print(\"Exists?\", os.path.isfile(audio_file_path))\n",
    "\n",
    "                os.environ[\"PATH\"] += os.pathsep + r\"C:\\Users\\ACER\\Downloads\\ffmpeg-master-latest-win64-gpl\\bin\"\n",
    "                print(\"PATH:\", os.environ[\"PATH\"])\n",
    "                \n",
    "                # Load Openai Whisper model\n",
    "                \"\"\"remember to install cuda version that matches your gpu: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\"\"\"\n",
    "                whisper_model = whisper.load_model(\"base\", device=\"cuda\")           \n",
    "                print(\"Model device:\", whisper_model.device)\n",
    "                \n",
    "                # Transcribe\n",
    "                cur_transcription = whisper_model.transcribe(audio_file_path, fp16=True)[\"text\"].strip()\n",
    "\n",
    "                export_to_github(title, cur_transcription)\n",
    "\n",
    "                return cur_transcription,title\n",
    "    else:\n",
    "        return check_trans.lower(),title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_cleaner(text):\n",
    "    \"\"\"Remove redundant characters that are not alphanumeric\n",
    "\n",
    "    Args:\n",
    "        text (str): input text\n",
    "    \"\"\"\n",
    "    pattern = r'[^a-zA-Z0-9\\s]'\n",
    "    text = re.sub(pattern, ' ', text).strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Remove stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stopwords\n",
    "\n",
    "    Args:\n",
    "        text (str): input text\n",
    "\n",
    "    Returns:\n",
    "        text (str): cleaned text without stopwords\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Preprocessing & Storage Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_document(transcription):\n",
    "    text = transcription\n",
    "    text_documents = [Document(page_content=text)]\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=50)\n",
    "    documents = text_splitter.split_documents(text_documents)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Embedding Model, Pine Cone DB & Store Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(\n",
    "    api_key=os.getenv(\"PINECONE_API_KEY\")\n",
    ")\n",
    "\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "def store_vector_db(title,documents):\n",
    "    title = title.replace(\"_\",\"-\")\n",
    "    index_name = f\"{title}-index\"\n",
    "    if index_name not in str(pc.list_indexes()):\n",
    "        pc.create_index(index_name,dimension = 1536, metric = \"cosine\", \n",
    "                        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "                        deletion_protection=\"disabled\")\n",
    "        index = pc.Index(host = pc.describe_index(index_name).host)\n",
    "        pinecone = PineconeVectorStore.from_documents(documents=documents, embedding=embeddings, index_name=index_name\n",
    "    )\n",
    "        return pinecone\n",
    "    else:\n",
    "        index = pc.Index(host = pc.describe_index(index_name).host)\n",
    "        pinecone = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "        return pinecone\n",
    "\n",
    "def reset_index(title):\n",
    "    index_name = f\"{title}-index\"\n",
    "    index = pc.Index(host= pc.describe_index(index_name).host)\n",
    "    index.delete_index(delete_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ExtractVideoTranscription(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        transcription, title = extract_transcription(X)\n",
    "        return {\"transcription\": transcription, \"title\": title}\n",
    "\n",
    "class CleanTranscription(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        trans = remove_stopwords(text_cleaner(X[\"transcription\"]))\n",
    "        return {\"transcription\": trans, \"title\": X[\"title\"]}\n",
    "\n",
    "class SplitDocuments(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        documents = split_document(X[\"transcription\"])\n",
    "        return {\"documents\": documents, \"title\": X[\"title\"]}\n",
    "\n",
    "class StoreVectorDB(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        title = X[\"title\"]\n",
    "        documents = X[\"documents\"]\n",
    "        store_vector_db(title,documents)\n",
    "        return {\"documents\": documents, \"title\": title}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Data Extraction & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for file data_trans/jensen_huang_--_nvidias_ceo_on_the_nex.txt...\n",
      "200\n",
      "Found file. Downloading...\n"
     ]
    }
   ],
   "source": [
    "preprocessing_pipeline = Pipeline([\n",
    "    (\"extract\", ExtractVideoTranscription()),\n",
    "    (\"clean\", CleanTranscription()),\n",
    "    (\"split\", SplitDocuments()),\n",
    "    (\"store\", StoreVectorDB()),\n",
    "])\n",
    "\n",
    "preprocessed_dict = preprocessing_pipeline.fit_transform(str(input(\"Enter a youtube url: \")))\n",
    "documents, title = preprocessed_dict.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent 1: Context-Based Querying Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=\"gpt-4o-mini\"\n",
    ")\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "translator = GoogleTranslator(source=\"vi\", target=\"en\")\n",
    "\n",
    "\"\"\"-----------------------------------------------------------------\"\"\"\n",
    "# define an query rewriting agent for current query + most recent chat history\n",
    "def retrieve_history(k=3):\n",
    "    k = k*2\n",
    "    history = memory.load_memory_variables({}).get(\"history\")[-k:]\n",
    "    pattern = r\"^(content=')(.*?)(' .*?)$\"\n",
    "    a=1\n",
    "    b=0\n",
    "    cleaned_dict={}\n",
    "    for idx,i in enumerate(history):\n",
    "        output = re.sub(pattern=pattern,string=str(i),repl=r\"\\2\")\n",
    "        name = \"Human\" if idx%2==0 else \"AI\"\n",
    "        question_no = a\n",
    "        b+=1\n",
    "        if b >= 2:\n",
    "            b=0\n",
    "            a+=1\n",
    "        if b==1:\n",
    "            cleaned_dict[f'Question {question_no}'] = {}\n",
    "        cleaned_dict[f'Question {question_no}'][name] = output\n",
    "    return cleaned_dict\n",
    "\n",
    "def rewrite_query(new_query):\n",
    "    current_hist = retrieve_history()\n",
    "    history = str(current_hist)\n",
    "    rewrite_prompt = [\n",
    "        SystemMessage(content=\"You are a helpful query rewriting assistant. Rewrite the user's query based on their recent history. Return only the rewritten query.\"),\n",
    "        HumanMessage(content=f\"History: {history}\\nNew query: {new_query}\")\n",
    "    ]\n",
    "    outcome = model.invoke(rewrite_prompt).content # may modify using a nicher model\n",
    "    print(f\"Rewritten Query: {outcome}\")\n",
    "    return outcome\n",
    "\n",
    "\"\"\"-----------------------------------------------------------------\"\"\"\n",
    "\n",
    "def retrieve_context(input):\n",
    "    try:\n",
    "        # Extract the content from the message object\n",
    "        if isinstance(input, dict) and \"question\" in input:\n",
    "            input = input[\"question\"][-1].content  # Extract human question only\n",
    "        elif isinstance(input, str):\n",
    "            input = input  # Directly use the string if provided\n",
    "        else:\n",
    "            raise ValueError(\"Input is not in a valid format.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing input: {e}\")\n",
    "        return None\n",
    "    \n",
    "    if detect(input) != \"en\":\n",
    "        query = translator.translate(input)\n",
    "        print(f\"Translated query: {query}\")\n",
    "    else:\n",
    "        query = input\n",
    "        print(f\"Original text already in English\")\n",
    "    \n",
    "    context = store_vector_db(title,documents) \\\n",
    "                .as_retriever(search_type = \"similarity_score_threshold\",search_kwargs={'score_threshold': 0.4}) \\\n",
    "                .get_relevant_documents(query)\n",
    "    compiled_docu = \" \".join([doc.page_content for doc in context])\n",
    "    print(\"Retrieved Context:\", compiled_docu, \"\\n-----\\n\")\n",
    "    return compiled_docu\n",
    "\n",
    "def query_from_context_main():\n",
    "    template = \"\"\"\n",
    "    Think step by step before answering the question based on the context below. If you can't find the answer in the context below, say that you don't know.\n",
    "\n",
    "    **Context:** {context}\n",
    "\n",
    "    **Question:** {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    rewriter = RunnableLambda(rewrite_query)\n",
    "\n",
    "    retriever_step = RunnableLambda(retrieve_context)\n",
    "    retriever = RunnableParallel(context = retriever_step,\n",
    "                        question=RunnablePassthrough(), \n",
    "                        #  language = RunnablePassthrough()\n",
    "                        )\n",
    "    if memory.load_memory_variables({}).get(\"history\") == []:\n",
    "        chain = retriever | prompt | model | parser\n",
    "    else:\n",
    "        chain = rewriter | retriever | prompt | model | parser\n",
    "    return chain\n",
    "\n",
    "\"\"\"-----------------------------------------------------------------\"\"\"\n",
    "\n",
    "def web_search_tool(query):\n",
    "    try:\n",
    "        query = query[\"question\"][1].content # Extract human question only\n",
    "    except Exception as e:\n",
    "        query = query\n",
    "    web_search_tool = TavilySearchResults(k=3)\n",
    "    results = web_search_tool.invoke(query)\n",
    "    extracted_results = \"\\n\".join([remove_stopwords(re.sub(r\"[^a-zA-Z0-9\\s.,!?]\",\"\",res[\"content\"].replace(\"\\n\",\" \").strip())) for res in results])\n",
    "    return extracted_results\n",
    "\n",
    "def alternative_web_search_context(prev_result):\n",
    "     if \"i don't know\" in prev_result.lower():\n",
    "          return True\n",
    "\n",
    "def build_chain_web():\n",
    "    rewriter = RunnableLambda(rewrite_query)\n",
    "\n",
    "    search_step = RunnableLambda(lambda q: web_search_tool(q))  # Search condition\n",
    "    retriever_web = RunnableParallel(context=search_step, question=RunnablePassthrough())\n",
    "    web_prompt = ChatPromptTemplate.from_template(\"Web search-based context:\\n{context}\\n\\n**Question:** {question}\")\n",
    "    if memory.load_memory_variables({}).get(\"history\") == []:\n",
    "        webchain = retriever_web | web_prompt | model | parser\n",
    "    else:\n",
    "        webchain = rewriter | retriever_web | web_prompt | model | parser\n",
    "    \n",
    "    return webchain\n",
    "\n",
    "\"\"\"-----------------------------------------------------------------\"\"\"\n",
    "\n",
    "def start_conversation():    \n",
    "    \n",
    "    system_message = SystemMessage(content=\"You are a truthful expert in extracting insights from provided context for QnA.\")\n",
    "    question_no = 0\n",
    "\n",
    "    memory.clear()\n",
    "    print(f\"Check history: {memory.load_memory_variables({}).get(\"history\")}\")\n",
    "\n",
    "    while True:\n",
    "        query = str(input(\"Ask me a question (type 'exit' to quit): \")).lower()\n",
    "\n",
    "        human_message = HumanMessage(content=query)\n",
    "        messages = [system_message,human_message]\n",
    "\n",
    "        if query == 'exit':\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "        \n",
    "        question_no += 1\n",
    "        print(f\"\\nQuestion {question_no}:\")\n",
    "        \n",
    "        chain_main = query_from_context_main()\n",
    "        main_result = []\n",
    "        for s in chain_main.stream({\"question\":messages}):\n",
    "            main_result.append(s)\n",
    "            print(s, end =\"\",flush=True)\n",
    "        \n",
    "        if alternative_web_search_context(\"\".join(main_result)):\n",
    "            feedback = str(input(\"Would you like to switch to web search tool? (yes/no): \"))\n",
    "            if feedback.lower() == 'yes':\n",
    "                print(f\"\\nSwitching to web search tool...\")\n",
    "                chain_web = build_chain_web()\n",
    "                for s in chain_web.stream({\"question\":messages}):\n",
    "                    main_result.append(s)\n",
    "                    print(s, end =\"\",flush=True)\n",
    "            else:\n",
    "                print(\" Please ask a different question.\")\n",
    "                pass\n",
    "        memory.chat_memory.add_user_message(query) # add user query to memory\n",
    "        memory.chat_memory.add_ai_message(\"\".join(main_result)) # add ai result to memory\n",
    "        print(\"\\n\",\"---\"*20,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.1 (20241206.2353)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"608pt\" height=\"560pt\"\n",
       " viewBox=\"0.00 0.00 608.47 559.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 555.5)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-555.5 604.47,-555.5 604.47,4 -4,4\"/>\n",
       "<!-- A -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>A</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"312.68\" cy=\"-533.5\" rx=\"80.01\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"312.68\" y=\"-528.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Start Conversation</text>\n",
       "</g>\n",
       "<!-- B -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>B</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"312.68\" cy=\"-460.5\" rx=\"64.66\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"312.68\" y=\"-455.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Check History</text>\n",
       "</g>\n",
       "<!-- A&#45;&gt;B -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>A&#45;&gt;B</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M312.68,-515.31C312.68,-507.73 312.68,-498.6 312.68,-490.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"316.18,-490.04 312.68,-480.04 309.18,-490.04 316.18,-490.04\"/>\n",
       "</g>\n",
       "<!-- C -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>C</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"130.68\" cy=\"-283.5\" rx=\"109.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"130.68\" y=\"-278.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Query from Context Main</text>\n",
       "</g>\n",
       "<!-- B&#45;&gt;C -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>B&#45;&gt;C</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M249.3,-456.32C202.57,-450.26 141.91,-433.78 110.68,-390 94.42,-367.2 104.13,-334.84 114.93,-311.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"117.95,-313.76 119.36,-303.26 111.71,-310.59 117.95,-313.76\"/>\n",
       "<text text-anchor=\"middle\" x=\"161.68\" y=\"-366.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">If History is Blank</text>\n",
       "</g>\n",
       "<!-- D -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>D</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"348.68\" cy=\"-283.5\" rx=\"74.89\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"348.68\" y=\"-278.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Build Chain Web</text>\n",
       "</g>\n",
       "<!-- B&#45;&gt;D -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>B&#45;&gt;D</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M368.15,-451C398.06,-445.17 430.67,-436.33 439.68,-424.5 447.04,-414.84 470.67,-369.87 434.68,-319.5 429.16,-311.77 421.62,-305.74 413.29,-301.05\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"415.09,-298.03 404.57,-296.79 412.02,-304.32 415.09,-298.03\"/>\n",
       "<text text-anchor=\"middle\" x=\"505.11\" y=\"-366.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">If History is Blank</text>\n",
       "</g>\n",
       "<!-- K -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>K</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"319.68\" cy=\"-372\" rx=\"97.93\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"319.68\" y=\"-366.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Query Rewriting Agent</text>\n",
       "</g>\n",
       "<!-- B&#45;&gt;K -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>B&#45;&gt;K</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M312.37,-442.3C312.35,-432.27 312.6,-419.4 313.68,-408 313.88,-405.93 314.13,-403.79 314.41,-401.65\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"317.85,-402.29 315.94,-391.87 310.94,-401.21 317.85,-402.29\"/>\n",
       "<text text-anchor=\"middle\" x=\"376.68\" y=\"-411.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">If History is Not Blank</text>\n",
       "</g>\n",
       "<!-- E -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>E</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"130.68\" cy=\"-195\" rx=\"73.87\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"130.68\" y=\"-189.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Retrieve Context</text>\n",
       "</g>\n",
       "<!-- C&#45;&gt;E -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>C&#45;&gt;E</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M130.68,-265.41C130.68,-253.76 130.68,-238.05 130.68,-224.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"134.18,-224.86 130.68,-214.86 127.18,-224.86 134.18,-224.86\"/>\n",
       "<text text-anchor=\"middle\" x=\"176.81\" y=\"-234.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Retrieve Context</text>\n",
       "</g>\n",
       "<!-- F -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>F</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"348.68\" cy=\"-195\" rx=\"74.38\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"348.68\" y=\"-189.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Web Search Tool</text>\n",
       "</g>\n",
       "<!-- D&#45;&gt;F -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>D&#45;&gt;F</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M348.68,-265.41C348.68,-253.76 348.68,-238.05 348.68,-224.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"352.18,-224.86 348.68,-214.86 345.18,-224.86 352.18,-224.86\"/>\n",
       "<text text-anchor=\"middle\" x=\"395.18\" y=\"-234.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Web Search Tool</text>\n",
       "</g>\n",
       "<!-- H -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>H</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"130.68\" cy=\"-106.5\" rx=\"77.45\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"130.68\" y=\"-101.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Print Main Result</text>\n",
       "</g>\n",
       "<!-- E&#45;&gt;H -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>E&#45;&gt;H</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M130.68,-176.91C130.68,-165.26 130.68,-149.55 130.68,-136.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"134.18,-136.36 130.68,-126.36 127.18,-136.36 134.18,-136.36\"/>\n",
       "<text text-anchor=\"middle\" x=\"179.43\" y=\"-145.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Print Main Result</text>\n",
       "</g>\n",
       "<!-- I -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>I</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"348.68\" cy=\"-106.5\" rx=\"73.87\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"348.68\" y=\"-101.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Print Web Result</text>\n",
       "</g>\n",
       "<!-- F&#45;&gt;I -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>F&#45;&gt;I</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M348.68,-176.91C348.68,-165.26 348.68,-149.55 348.68,-136.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"352.18,-136.36 348.68,-126.36 345.18,-136.36 352.18,-136.36\"/>\n",
       "<text text-anchor=\"middle\" x=\"394.81\" y=\"-145.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Print Web Result</text>\n",
       "</g>\n",
       "<!-- G -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>G</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"130.68\" cy=\"-18\" rx=\"130.68\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"130.68\" y=\"-12.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Alternative Web Search Context</text>\n",
       "</g>\n",
       "<!-- G&#45;&gt;I -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>G&#45;&gt;I</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M233.92,-29.3C257.22,-34.52 281.13,-42.33 301.68,-54 312.85,-60.34 322.83,-70.27 330.7,-79.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"327.71,-81.59 336.62,-87.32 333.23,-77.29 327.71,-81.59\"/>\n",
       "<text text-anchor=\"middle\" x=\"381.07\" y=\"-57.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Switch to Web Search</text>\n",
       "</g>\n",
       "<!-- H&#45;&gt;G -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>H&#45;&gt;G</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M130.68,-88.41C130.68,-76.76 130.68,-61.05 130.68,-47.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"134.18,-47.86 130.68,-37.86 127.18,-47.86 134.18,-47.86\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.93\" y=\"-57.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Check Alternative Web Search</text>\n",
       "</g>\n",
       "<!-- J -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>J</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"487.68\" cy=\"-533.5\" rx=\"77.45\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"487.68\" y=\"-528.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">End Conversation</text>\n",
       "</g>\n",
       "<!-- H&#45;&gt;J -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>H&#45;&gt;J</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M196.9,-116.17C218.79,-118.95 243.27,-121.96 265.68,-124.5 305.41,-129 411.38,-120.37 444.68,-142.5 458.03,-151.37 585.75,-280.6 559.68,-390 549.32,-433.47 523.46,-478.69 505.73,-506.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"503.01,-504 500.44,-514.28 508.86,-507.84 503.01,-504\"/>\n",
       "</g>\n",
       "<!-- I&#45;&gt;J -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>I&#45;&gt;J</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M416.22,-114.14C439.16,-119.1 463.75,-127.68 482.68,-142.5 566.94,-208.45 571.31,-250.3 597.68,-354 613.38,-415.73 556.23,-476.46 518.48,-508.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"516.44,-505.87 510.99,-514.95 520.92,-511.25 516.44,-505.87\"/>\n",
       "</g>\n",
       "<!-- K&#45;&gt;C -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>K&#45;&gt;C</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M247.13,-359.66C225.81,-354.42 203.03,-346.86 183.68,-336 171.86,-329.37 160.77,-319.43 151.82,-310.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"154.57,-307.89 145.26,-302.84 149.39,-312.59 154.57,-307.89\"/>\n",
       "<text text-anchor=\"middle\" x=\"255.68\" y=\"-322.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Query from Context Main</text>\n",
       "</g>\n",
       "<!-- K&#45;&gt;D -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>K&#45;&gt;D</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M325.41,-353.91C329.36,-342.14 334.69,-326.23 339.26,-312.61\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"342.52,-313.88 342.38,-303.29 335.89,-311.65 342.52,-313.88\"/>\n",
       "<text text-anchor=\"middle\" x=\"383.65\" y=\"-322.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Build Chain Web</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x22f69e52f00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "from IPython.display import display\n",
    "\n",
    "def visualize_flow():\n",
    "    dot = Digraph(comment='Code Flow')\n",
    "\n",
    "    # Define nodes\n",
    "    dot.node('A', 'Start Conversation')\n",
    "    dot.node('B', 'Check History')\n",
    "    dot.node('C', 'Query from Context Main')\n",
    "    dot.node('D', 'Build Chain Web')\n",
    "    dot.node('E', 'Retrieve Context')\n",
    "    dot.node('F', 'Web Search Tool')\n",
    "    dot.node('G', 'Alternative Web Search Context')\n",
    "    dot.node('H', 'Print Main Result')\n",
    "    dot.node('I', 'Print Web Result')\n",
    "    dot.node('J', 'End Conversation')\n",
    "    dot.node('K', 'Query Rewriting Agent')\n",
    "\n",
    "    # Define edges\n",
    "    dot.edges(['AB'])\n",
    "    dot.edge('B', 'C', label='If History is Blank')\n",
    "    dot.edge('B', 'D', label='If History is Blank')\n",
    "    dot.edge('B', 'K', label='If History is Not Blank')\n",
    "    dot.edge('K', 'C', label='Query from Context Main')\n",
    "    dot.edge('K', 'D', label='Build Chain Web')\n",
    "    dot.edge('C', 'E', label='Retrieve Context')\n",
    "    dot.edge('D', 'F', label='Web Search Tool')\n",
    "    dot.edge('E', 'H', label='Print Main Result')\n",
    "    dot.edge('F', 'I', label='Print Web Result')\n",
    "    dot.edge('H', 'G', label='Check Alternative Web Search')\n",
    "    dot.edge('G', 'I', label='Switch to Web Search', constraint='false')\n",
    "    dot.edge('H', 'J', constraint='false')\n",
    "    dot.edge('I', 'J', constraint='false')\n",
    "\n",
    "    return dot\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + r'C:\\Program Files\\Graphviz\\bin'\n",
    "dot = visualize_flow()\n",
    "display(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check history: []\n",
      "\n",
      "Question 1:\n",
      "Original text already in English\n",
      "Retrieved Context: jensen first time human history producing manufacturing intelligence like production raw material comes lot genius goes box comes intelligence thats refined lukas youre listening gradient dissent show machine learning real world im host lukas biewald today gradient dissent interviewed guest ive looking forward interviewing quite long time jensen huang ceo founder nvidia youve trained machine learning model youve probably trained nvidia hardware get machine learning talk views future holds super fun interview really hope enjoy lukas right well thanks much collected questions community ton theres questions im sure get im going get questions first jensen okay lukas wanted start number one question wanted ask ive always wondered think almost everyone training machine learning models days uses manufacturing lukas well thanks much thats really kind im touched appreciate jensen keep great work lukas youre enjoying interviews want learn please click link show notes description find links papers mentioned supplemental material transcription work really hard produce check write little code put something together jensen long time get tinker people wonderful thing nvidia 24000 people could tinker little something everybody amount tinkering thats going around company incredible theres phrase say reach friends really see way reach friends company brainstorm little something go try something somebody else theyre brainstorming try something thats guess tinkering scale lukas thats super cool love another question lot people askim curious people originally think nvidia games gamer play video games jensen havent played much games see almost every game goes get benefit collaboration every game company world theyre labs people tell ill run go check play bit last time probablyone favorite games battlefield first came kids teenagers home coming gaming age three us really need tapeout bonus achievement bonus everybodys trying best thats one example lukas thats great one else youve got others id love hear jensen okay heres another one well want diplomatic well theres many ceos could using techniques hate critical criticism style tend one ones theres anything need say tend like say team group working hearing things im hearing things everybody else hearing things instead translated lukas interesting thats really unusual perspective think lot people think absolutely must one ones across company think like reports jensen dont dont many leaders dont criticize dont reason probably important ceos becauseyou cant eliminate completely want reduce amount jensen told jensen told way somehow steer conversation otherwise done merits instead somehow translated \n",
      "-----\n",
      "\n",
      "Jensen Huang is the CEO and founder of NVIDIA.\n",
      " ------------------------------------------------------------ \n",
      "\n",
      "\n",
      "Question 2:\n",
      "Rewritten Query: Based on your recent query about Jensen Huang, you might be asking for his age. Would you like to know how old Jensen Huang is?\n",
      "Original text already in English\n",
      "Retrieved Context: jensen first time human history producing manufacturing intelligence like production raw material comes lot genius goes box comes intelligence thats refined lukas youre listening gradient dissent show machine learning real world im host lukas biewald today gradient dissent interviewed guest ive looking forward interviewing quite long time jensen huang ceo founder nvidia youve trained machine learning model youve probably trained nvidia hardware get machine learning talk views future holds super fun interview really hope enjoy lukas right well thanks much collected questions community ton theres questions im sure get im going get questions first jensen okay lukas wanted start number one question wanted ask ive always wondered think almost everyone training machine learning models days uses amazing thing producing intelligence first time human history producing manufacturing intelligence like production raw material comes lot genius goes box comes intelligence thats refined large companies depending us ai intelligence manufactured large scales teams working really really hard keep demand lukas youve running nvidia quite long time curious feel youve changed leader decades running company jensen know youre almost asking wrong person could ask almost anybody else around lukas fair enough experience changed jensen thats easier question 30 years old didnt know anything ceo lot learning job many management techniques really dumb dont use anymore lukas like jensen well alright ill give couple lukas awesome thank jensen list dumb things ive done years quite large could write book write little code put something together jensen long time get tinker people wonderful thing nvidia 24000 people could tinker little something everybody amount tinkering thats going around company incredible theres phrase say reach friends really see way reach friends company brainstorm little something go try something somebody else theyre brainstorming try something thats guess tinkering scale lukas thats super cool love another question lot people askim curious people originally think nvidia games gamer play video games jensen havent played much games see almost every game goes get benefit collaboration every game company world theyre labs people tell ill run go check play bit last time probablyone favorite games battlefield first came kids teenagers home coming gaming age three us jensen first one greatest contributions industry democratized scientific computing nvidia gpus breakthroughs alexnet wasnt supercomputer cloud geforce card simultaneously researchers around world buying geforce gpus architecturally theyre supercomputers building able use discover nextthe breakthrough enjoying today thing happening many different fields im really proud fact weve democratized high performance computing put hands researcher dont go get gigantic funds able research one scientists quantum chemistry said one day learned son working one computer companies silicon valley go buy gaming cards download cuda sdk port quantum chemistry software running ibm supercomputer onto gaming gpu amazed fast wait rest week supercomputer finish could compare results went bought many gpus could \n",
      "-----\n",
      "\n",
      "I don't know.\n",
      "Switching to web search tool...\n",
      "Rewritten Query: Based on your recent inquiry about Jensen Huang, you might be asking: \"How old is Jensen Huang?\"\n",
      "Jensen Huang was born on February 17, 1963. As of now, he is 60 years old.\n",
      " ------------------------------------------------------------ \n",
      "\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "start_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent 2: (Advanced) Summarization Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/18/2024 11:40:31 INFO Loaded model pszemraj/led-large-book-summary to cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from textsum.summarize import Summarizer\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "model_name = \"pszemraj/led-large-book-summary\"\n",
    "summarizer = Summarizer(\n",
    "    model_name_or_path=model_name\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "translator = GoogleTranslator(source=\"vi\", target=\"en\")\n",
    "\n",
    "def fetch_list_pinecone(title) -> pd.DataFrame:\n",
    "    # Define index\n",
    "    title_refined = title.replace(\"_\",\"-\")\n",
    "    index_name = f\"{title_refined}-index\"\n",
    "\n",
    "    index = pc.Index(name = index_name, host = pc.describe_index(index_name).host)\n",
    "\n",
    "    # Define vector_id\n",
    "    id_list = []\n",
    "    for id in index.list():\n",
    "        for i in id:\n",
    "            id_list.append(i)\n",
    "\n",
    "    # Fetch vector list including metadata\n",
    "    fetch_list = index.fetch(ids=id_list)\n",
    "\n",
    "    # Return df of id, text, vector values\n",
    "    list_vec = []\n",
    "    for key, content in fetch_list[\"vectors\"].items():\n",
    "        id = key\n",
    "        text = content[\"metadata\"][\"text\"]\n",
    "        values = content[\"values\"]\n",
    "        list_vec.append([id,text,values])\n",
    "    \n",
    "    df_vec_extracted = pd.DataFrame(list_vec, columns=[\"id\", \"text\", \"values\"])\n",
    "\n",
    "    return df_vec_extracted\n",
    "\n",
    "def preprocess_run_kmeans(df_vec_extracted, n = 8) -> pd.DataFrame:\n",
    "    # Split vector values to a separate df to run kmeans\n",
    "    df_vec_text = df_vec_extracted.iloc[:,:2]\n",
    "\n",
    "    df_vec_val = df_vec_extracted.loc[:,[\"id\",\"values\"]]\n",
    "    df_vec_val = pd.concat([df_vec_val[[\"id\"]], pd.DataFrame(df_vec_val[\"values\"].tolist())],axis=1)    \n",
    "\n",
    "    n_clusters = n\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    df_vec_val[\"cluster\"] = kmeans.fit_predict(df_vec_val.iloc[:, 1:])\n",
    "    df_vec_text_clustered = pd.merge(df_vec_text,df_vec_val[\"cluster\"],right_index=True,left_index=True)\n",
    "    \n",
    "    # Return final clustered results in df form\n",
    "    clustered_texts = df_vec_text_clustered.groupby(\"cluster\")[\"text\"].apply(\" \".join).reset_index()\n",
    "    return clustered_texts\n",
    "\n",
    "def summarize_text(input):\n",
    "    out_str = summarizer.summarize_string(input)\n",
    "    return out_str\n",
    "\n",
    "def pre_summarize_chunk(title):\n",
    "    df_vec_extracted = fetch_list_pinecone(title)\n",
    "    clustered_texts = preprocess_run_kmeans(df_vec_extracted, n = 8)\n",
    "    clustered_texts[\"summarized_text\"] = clustered_texts[\"text\"].apply(lambda x: summarize_text(x))\n",
    "    input_sum=\"; \".join(clustered_texts[\"summarized_text\"].values)\n",
    "    return input_sum\n",
    "\n",
    "def summarize_main(title=title):\n",
    "    summarize_temp = \"\"\"\n",
    "    **Summarize the main points** and organize the information into a coherent summary based on the following context:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Ensure the summary is concise, clear, and covers all key details.\n",
    "    \"\"\"\n",
    "\n",
    "    summarize_prompt = ChatPromptTemplate.from_template(summarize_temp)\n",
    "\n",
    "    input_sum = pre_summarize_chunk(title)\n",
    "\n",
    "    # summarization chain of actions\n",
    "    sum_chain = summarize_prompt | model | parser\n",
    "\n",
    "    sum_chunk = []\n",
    "\n",
    "    for s in sum_chain.stream(input_sum):\n",
    "        sum_chunk.append(s)\n",
    "        print(s, end=\"\", flush=True)\n",
    "    return \"\".join(sum_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f458b0855d4450b69cf06d6bd8912c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a77069a333849de996796d7f4272c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f930bf1247d64e4e82f526c7206e9d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9c51c4f3534a89bb2661d62a5b91a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b9d483113f403bad4fe16816abcac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a15f30d5810427b8142674514eb3b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041178b609e1471695a86ce2451f7ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7a4a77d8e4404199c32d68f0a981dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/18/2024 11:41:53 INFO HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chapter emphasizes the concept of \"networks\" as fundamental to understanding daily life and its various elements, particularly in the fields of artificial intelligence and machine learning. It begins by listing the top 10 significant developments in technology, focusing on their interrelationships and collective impact on our lives. Key advancements discussed include:\n",
      "\n",
      "1. The advent of affordable semiconductor chips, facilitating computational advances.\n",
      "2. Enhanced speed and power in data processing.\n",
      "3. A shift from brute force methods to logic, statistics, and synthetic biology.\n",
      "4. The innovation of the floppy drive, enabling individuals to build homemade supercomputers.\n",
      "\n",
      "The narrative shifts to the industrial revolution’s next phase, highlighting the role of machine learning and artificial intelligence in transforming work and thought processes. It references the pioneering work of Jensen and his company in creating new semiconductor technologies and algorithms that enhance computing capabilities.\n",
      "\n",
      "Lucas expresses particular interest in future applications, such as virtual reality, and mentions his creation—a robot with an avatar capable of language comprehension. The author reflects on the transformative power of machine learning, which turns raw data into neural networks, revolutionizing software development and enabling engineers to innovate across diverse fields like agriculture, medicine, and architecture.\n",
      "\n",
      "Despite the excitement around these advancements, the author critiques the inefficacy of concepts like \"tapeout bonuses,\" which aim to reward employee efforts but often fail due to resource limitations in companies. He argues that successful innovation should focus on creating substantial value rather than merely achieving minimum viable products. Ultimately, the chapter explores how interconnected technological developments are reshaping our understanding of productivity and progress in various domains."
     ]
    }
   ],
   "source": [
    "output = summarize_main(title);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
